{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810b057c-be6b-4fec-b01c-02b2654b497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ec0cf4-8dfc-420f-9b2f-e9b23d61ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b8056e-fc12-4db0-99d5-a408c39a3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn  # Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from flax import struct                # Flax dataclasses\n",
    "import optax                           # Common loss functions and optimizers\n",
    "\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "import gymnax\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091580b0-dd1f-4249-891d-9d8dd5c8b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.base_model import BaseModel\n",
    "\n",
    "# from agents.REINFORCE_discrete import ActorCriticDiscrete\n",
    "from agents.REINFORCE import REINFORCEDiscrete\n",
    "from agents.advantage_estimator import state_value_estimator, gae_estimator\n",
    "\n",
    "from trainer import build_trainer\n",
    "\n",
    "from utils.callbacks import versatile_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394865f6-13b6-4aae-a703-fa3f13bad79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "\n",
    "discount = 0.99\n",
    "\n",
    "num_envs = 8\n",
    "\n",
    "iters = 2000\n",
    "\n",
    "max_episode_steps = 500\n",
    "\n",
    "env_name = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e7bb76-b55a-4180-bc44-f4c2be83dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, env_params = gymnax.make(env_name)\n",
    "\n",
    "obs_size = env.observation_space(env_params).shape\n",
    "action_num = env.action_space(env_params).n\n",
    "action_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d526b08-acb9-4d46-a12d-bd9c160a5e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1123/2000 [00:45<00:33, 26.44it/s, episode_reward=500.0]    "
     ]
    }
   ],
   "source": [
    "cpu_device = jax.devices('cpu')[0]\n",
    "gpu_device = jax.devices('gpu')[0]\n",
    "\n",
    "with jax.default_device(gpu_device):\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    dummy = jnp.ones([1, *obs_size])\n",
    "    actor_init_key, critic_init_key, key = jax.random.split(key, 3)\n",
    "    \n",
    "    actor_model = nn.Sequential([BaseModel(), nn.Dense(features = action_num)])\n",
    "    actor_params = actor_model.init(actor_init_key, dummy)['params']\n",
    "    actor_tx = optax.adam(actor_lr)\n",
    "    actor = TrainState.create(apply_fn=actor_model.apply,\n",
    "                              params=actor_params,\n",
    "                              tx=actor_tx,\n",
    "                              )\n",
    "    \n",
    "    critic_model = nn.Sequential([BaseModel(), nn.Dense(features = 1)])\n",
    "    critic_params = critic_model.init(critic_init_key, dummy)['params']\n",
    "    critic_tx = optax.adam(critic_lr)\n",
    "    critic = TrainState.create(apply_fn=critic_model.apply,\n",
    "                              params=critic_params,\n",
    "                              tx=critic_tx,\n",
    "                              )\n",
    "    \n",
    "    agent = REINFORCEDiscrete(gae_estimator(discount, 0.95))\n",
    "    \n",
    "    callback = versatile_callback(iters, tqdm_keys = [\"episode_reward\"])\n",
    "    \n",
    "    fori_body = build_trainer(agent, env, env_params, num_envs, obs_size, action_size, max_episode_steps, callback)\n",
    "    \n",
    "    carry = (actor, critic, key)\n",
    "    carry = jax.lax.fori_loop(0, iters, fori_body, carry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c4a4b-e38d-492c-a710-9e59355f3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(callback.history[\"episode_reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903aec60-589b-4aff-b922-33e7f5b901f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.render import save_frames_as_gif\n",
    "\n",
    "from IPython.display import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import gym\n",
    "\n",
    "actor = carry[0]\n",
    "\n",
    "gym_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "obs, _ = gym_env.reset()\n",
    "frames = []\n",
    "for t in tqdm(range(500)):\n",
    "    frames.append(gym_env.render())\n",
    "    action = agent.suggest_action(jnp.expand_dims(obs, 0), actor)[0]\n",
    "    obs, _, done, _, _ = gym_env.step(action.item())\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "gym_env.close()\n",
    "\n",
    "save_frames_as_gif(frames, filename = \"tmp.gif\")\n",
    "\n",
    "Image(open('tmp.gif','rb').read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
