{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810b057c-be6b-4fec-b01c-02b2654b497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ec0cf4-8dfc-420f-9b2f-e9b23d61ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/notebooks/JAXGym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b8056e-fc12-4db0-99d5-a408c39a3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn  # Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from flax import struct                # Flax dataclasses\n",
    "import optax                           # Common loss functions and optimizers\n",
    "\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "import gymnax\n",
    "import brax\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091580b0-dd1f-4249-891d-9d8dd5c8b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.base_model import BaseModel, NormalDistPredictor\n",
    "\n",
    "# from agents.REINFORCE_continuous import ActorCriticContinuous\n",
    "from agents.PPO_v2 import PPOContinuous\n",
    "from agents.advantage_estimator import gae_estimator\n",
    "\n",
    "from trainer_v2 import build_trainer\n",
    "\n",
    "from utils.callbacks import versatile_callback_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394865f6-13b6-4aae-a703-fa3f13bad79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-4\n",
    "\n",
    "discount = 0.99\n",
    "\n",
    "clip_ratio = 0.2\n",
    "ppo_steps = 4\n",
    "\n",
    "num_envs = 512\n",
    "\n",
    "update_iters = 50000\n",
    "\n",
    "train_rollout_steps = 20\n",
    "eval_rollout_steps = 1000\n",
    "eval_every = 400\n",
    "\n",
    "env_name = \"hopper\"\n",
    "backend = \"positional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "417b6f2a-e1fe-4f2c-a064-40ee2ab2265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/luchris429/purejaxrl/blob/5343613b07b3bc543c49695df601fc40f5ec3062/purejaxrl/wrappers.py#L117\n",
    "\n",
    "from gymnax.environments import environment, spaces\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "\n",
    "class BraxGymnaxWrapper:\n",
    "    def __init__(self, env_name, backend=\"positional\"):\n",
    "        env = envs.get_environment(env_name=env_name, backend=backend)\n",
    "        env = EpisodeWrapper(env, episode_length=1000, action_repeat=1)\n",
    "        env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = (env.observation_size,)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        next_state = self._env.step(state, action)\n",
    "        return next_state.obs, next_state, next_state.reward, next_state.done > 0.5, {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self._env.observation_size,),\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self._env.action_size,),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e7bb76-b55a-4180-bc44-f4c2be83dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brax import envs\n",
    "\n",
    "env, env_params = BraxGymnaxWrapper(env_name), None\n",
    "\n",
    "obs_size = env.observation_space(env_params).shape\n",
    "action_size = env.action_space(env_params).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d526b08-acb9-4d46-a12d-bd9c160a5e80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 5524/50000 [05:07<28:18, 26.18it/s, episode_reward=1245.9813]  "
     ]
    }
   ],
   "source": [
    "cpu_device = jax.devices('cpu')[0]\n",
    "gpu_device = jax.devices('gpu')[0]\n",
    "\n",
    "steps = ppo_steps * update_iters\n",
    "# lr_scheduler = optax.linear_schedule(\n",
    "#   init_value=5e-4,\n",
    "#   end_value=5e-4,\n",
    "#   transition_steps=steps,\n",
    "# )\n",
    "\n",
    "with jax.default_device(gpu_device):\n",
    "    key = jax.random.PRNGKey(64)\n",
    "    \n",
    "    dummy = jnp.ones([1, *obs_size])\n",
    "    actor_init_key, critic_init_key, key = jax.random.split(key, 3)\n",
    "    \n",
    "    actor_model = nn.Sequential([BaseModel(hidden_size = 128, num_layers = 2), NormalDistPredictor(output_size = action_size, logvar_init_value = 0, limits = (-1, 1))])\n",
    "    actor_params = actor_model.init(actor_init_key, dummy)['params']\n",
    "    # actor_tx = optax.adam(lr_scheduler)\n",
    "    actor_tx = optax.adam(actor_lr)\n",
    "    actor = TrainState.create(apply_fn=actor_model.apply,\n",
    "                              params=actor_params,\n",
    "                              tx=actor_tx,\n",
    "                              )\n",
    "    \n",
    "    critic_model = nn.Sequential([BaseModel(hidden_size = 128, num_layers = 2), nn.Dense(features = 1)])\n",
    "    critic_params = critic_model.init(critic_init_key, dummy)['params']\n",
    "    # critic_tx = optax.adam(lr_scheduler)\n",
    "    critic_tx = optax.adam(critic_lr)\n",
    "    critic = TrainState.create(apply_fn=critic_model.apply,\n",
    "                              params=critic_params,\n",
    "                              tx=critic_tx,\n",
    "                              )\n",
    "    \n",
    "    agent = PPOContinuous(gae_estimator(discount, 0.95),\n",
    "                          clip_ratio, ppo_steps)\n",
    "    \n",
    "    callback = versatile_callback_v2(update_iters, tqdm_keys = [\"episode_reward\"], split_train_eval = True)\n",
    "\n",
    "    train = build_trainer(agent, env, env_params, num_envs, obs_size, action_size, \n",
    "                            train_rollout_steps, eval_rollout_steps, eval_every, \n",
    "                            callback, \n",
    "                            )\n",
    "    \n",
    "    actor, critic = train(update_iters, actor, critic, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c4a4b-e38d-492c-a710-9e59355f3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.gca().set_aspect('auto')\n",
    "plt.plot(callback.eval_history[\"episode_reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672988d-71dd-46ce-8356-133bb8f27e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "from brax.io import html\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# actor = carry[0]\n",
    "\n",
    "env = envs.create(env_name=env_name, backend=backend)\n",
    "\n",
    "jit_env_reset = jax.jit(env.reset)\n",
    "jit_env_step = jax.jit(env.step)\n",
    "jit_inference_fn = jax.jit(lambda obs: agent.suggest_action(obs, actor))\n",
    "\n",
    "total_rewards = 0\n",
    "\n",
    "rollout = []\n",
    "rng = jax.random.PRNGKey(seed=1)\n",
    "state = jit_env_reset(rng=rng)\n",
    "for _ in tqdm(range(1000)):\n",
    "    rollout.append(state.pipeline_state)\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    act = jit_inference_fn(state.obs)\n",
    "    state = jit_env_step(state, act)\n",
    "\n",
    "    if state.done:\n",
    "        print(\"Done at\", _)\n",
    "\n",
    "    # print(state.done, state.reward)\n",
    "\n",
    "    total_rewards += state.reward\n",
    "\n",
    "print(total_rewards)\n",
    "\n",
    "HTML(html.render(env.sys.tree_replace({'opt.timestep': env.dt}), rollout))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
